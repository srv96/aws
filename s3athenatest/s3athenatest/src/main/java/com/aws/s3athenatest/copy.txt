

import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericDatumWriter;
import org.apache.avro.file.DataFileWriter;
import org.apache.avro.io.DatumWriter;
import org.apache.parquet.avro.AvroParquetWriter;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.parquet.hadoop.metadata.CompressionCodecName;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.util.List;

public class ParquetConverter {

    public static InputStream convertListToParquetInputStream(List<String> dataList, Schema schema) throws IOException {
        ByteArrayOutputStream outputStream = new ByteArrayOutputStream();

        try (DataFileWriter<GenericData.Record> avroWriter = new DataFileWriter<>(new GenericDatumWriter<>(schema))) {
            avroWriter.create(schema, outputStream);
            for (String data : dataList) {
                GenericData.Record record = new GenericData.Record(schema);
                record.put("data", data);
                avroWriter.append(record);
            }
        }

        return new ByteArrayInputStream(outputStream.toByteArray());
    }

    // Example usage
    public static void main(String[] args) {
        List<String> dataList = Arrays.asList("String 1", "String 2", "String 3");
        String avroSchemaJson = "{\"type\":\"record\",\"name\":\"DataRecord\",\"fields\":[{\"name\":\"data\",\"type\":\"string\"}]}";
        Schema avroSchema = new Schema.Parser().parse(avroSchemaJson);

        try {
            InputStream parquetInputStream = convertListToParquetInputStream(dataList, avroSchema);
            // Use the parquetInputStream as needed
            // For example, you can write it to a file or send it over a network.
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}
