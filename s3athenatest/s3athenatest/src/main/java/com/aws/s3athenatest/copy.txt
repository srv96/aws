import org.apache.parquet.example.data.Group;
import org.apache.parquet.example.data.simple.SimpleGroupFactory;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.parquet.hadoop.example.GroupWriteSupport;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.util.List;

public class ParquetConverter {

    public static InputStream convertListToParquetInputStream(List<String> dataList, String schema) throws IOException {
        SimpleGroupFactory groupFactory = new SimpleGroupFactory(GroupWriteSupport.getSchemaFromString(schema));
        ByteArrayOutputStream outputStream = new ByteArrayOutputStream();

        try (ParquetWriter<Group> writer = new ParquetWriter<>(new ParquetWriter.Builder<>(new org.apache.hadoop.fs.Path("dummy"), groupFactory), new GroupWriteSupport())) {
            for (String data : dataList) {
                Group group = groupFactory.newGroup();
                group.append("data", data);
                writer.write(group);
            }
        }

        return new ByteArrayInputStream(outputStream.toByteArray());
    }

    // Example usage
    public static void main(String[] args) {
        List<String> dataList = Arrays.asList("String 1", "String 2", "String 3");
        String parquetSchema = "message data { required binary data; }";

        try {
            InputStream parquetInputStream = convertListToParquetInputStream(dataList, parquetSchema);
            // Use the parquetInputStream as needed
            // For example, you can write it to a file or send it over a network.
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}
